{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T22:46:50.460493Z",
     "start_time": "2024-05-04T22:46:50.452158Z"
    }
   },
   "source": [
    "# Управление с непрерывными действиями (Continuous Control) (<span style=\"color: green\"> 10 баллов за основную часть + 5 баллов за бонусную часть</span>)\n",
    "\n",
    "#### Дедлайн (жёсткий) задания: 04.05.2025,  UTC+3\n",
    "\n",
    "#### При сдаче задания нужно  поместить в архив данный файл и папки с логами и видео, сохраняя относительные пути, и послать архив в систему сдачи.\n",
    "\n",
    "### <span style=\"color: red\"> Если работа была списана и/или сделана LLM, то за работу ставится 0 баллов. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании предлагается решить задачу управления с непрерывными действиями, используя алгоритмы:\n",
    "\n",
    "- **Twin Delayed DDPG (TD3)** (**Раздел 6.1.6 (Алгоритм 23)**)\n",
    "- **Soft Actor-Critic (SAC)** (**Раздел 6.2.4 (Алгоритм 24)**)\n",
    "\n",
    "Оба алгоритма являются off-policy и считаются одними из наиболее эффективных для задач управления в непрерывном пространстве действий. Они основаны на базовом алгоритме **Deep Deterministic Policy Gradient (DDPG)** (**Раздел 6.1.5 (Алгоритм 22)**), который можно представить как \"DQN с отдельной нейросетью для аппроксимации жадной политики\". Основные отличия заключаются в различных стабилизационных приёмах:\n",
    "\n",
    "- TD3 обучает детерминированную политику, тогда как SAC использует стохастическую политику. Это означает, что в SAC достаточно просто сэмплировать действия из политики для исследования, в то время как в TD3 необходимо вручную добавлять шум к действиям.\n",
    "- TD3 добавляет к действиям обрезанный шум (clipped noise) при расчёте целевых значений, что помогает бороться с переоценкой. SAC использует формализм максимальной энтропии и добавляет бонус за энтропию в функцию ценности, поощряя более разнообразные действия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: Вицын Семён Сергеевич, Б05-131."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.167674Z",
     "start_time": "2025-04-24T08:13:43.161443Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:13:41.071306Z",
     "iopub.status.busy": "2025-04-24T01:13:41.070167Z",
     "iopub.status.idle": "2025-04-24T01:13:42.613909Z",
     "shell.execute_reply": "2025-04-24T01:13:42.612365Z",
     "shell.execute_reply.started": "2025-04-24T01:13:41.071262Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "from queue import deque\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from logger import TensorboardSummaries as Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.374789Z",
     "start_time": "2025-04-24T08:13:43.369879Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:13:42.617100Z",
     "iopub.status.busy": "2025-04-24T01:13:42.616042Z",
     "iopub.status.idle": "2025-04-24T01:13:42.654186Z",
     "shell.execute_reply": "2025-04-24T01:13:42.653427Z",
     "shell.execute_reply.started": "2025-04-24T01:13:42.617008Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Среда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы создадим экземпляр среды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.931381Z",
     "start_time": "2025-04-24T08:13:43.916456Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:22.807235Z",
     "iopub.status.busy": "2025-04-24T01:15:22.806395Z",
     "iopub.status.idle": "2025-04-24T01:15:23.521031Z",
     "shell.execute_reply": "2025-04-24T01:15:23.519471Z",
     "shell.execute_reply.started": "2025-04-24T01:15:22.807188Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность вектора состояний dim = 24\n",
      "n_actions = 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"Размерность вектора состояний dim =\", state_dim)\n",
    "print(\"n_actions =\", action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на среду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:44.371255Z",
     "start_time": "2025-04-24T08:13:44.271132Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:25.764899Z",
     "iopub.status.busy": "2025-04-24T01:15:25.764186Z",
     "iopub.status.idle": "2025-04-24T01:15:25.998916Z",
     "shell.execute_reply": "2025-04-24T01:15:25.997383Z",
     "shell.execute_reply.started": "2025-04-24T01:15:25.764856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на случайную политику и посмотрим, как это выглядит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:44.637836Z",
     "start_time": "2025-04-24T08:13:44.636248Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:28.224609Z",
     "iopub.status.busy": "2025-04-24T01:15:28.223860Z",
     "iopub.status.idle": "2025-04-24T01:15:28.255051Z",
     "shell.execute_reply": "2025-04-24T01:15:28.253632Z",
     "shell.execute_reply.started": "2025-04-24T01:15:28.224565Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomActor():\n",
    "    def get_action(self, states):\n",
    "        assert len(states.shape) == 1, \"Не работает с батчами\"\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:45.835552Z",
     "start_time": "2025-04-24T08:13:44.806511Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:28.905407Z",
     "iopub.status.busy": "2025-04-24T01:15:28.904703Z",
     "iopub.status.idle": "2025-04-24T01:15:31.849190Z",
     "shell.execute_reply": "2025-04-24T01:15:31.847725Z",
     "shell.execute_reply.started": "2025-04-24T01:15:28.905366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done:  1599\n",
      "done:  1685\n",
      "done:  1746\n",
      "done:  1806\n",
      "done:  3406\n",
      "done:  5006\n",
      "done:  6606\n",
      "done:  6679\n",
      "done:  8279\n",
      "done:  8335\n",
      "done:  9935\n"
     ]
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "rewards_per_step = []\n",
    "actor = RandomActor()\n",
    "\n",
    "for i in range(10000):\n",
    "    a = actor.get_action(s)\n",
    "    s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "    rewards_per_step.append(r)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "        print(\"done: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основном, каждый эпизод длится **1600 шагов** &mdash; это ограничение по времени, после которого происходит завершение эпизода. Однако иногда эпизод завершается раньше, если симуляция \"понимает\", что агент, например, **упал** или **разбился** &mdash; то есть ситуация явно неудачная для продолжения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что мы получаем при использовании случайной политики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:45.839119Z",
     "start_time": "2025-04-24T08:13:45.836680Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:33.108058Z",
     "iopub.status.busy": "2025-04-24T01:15:33.107324Z",
     "iopub.status.idle": "2025-04-24T01:15:33.155155Z",
     "shell.execute_reply": "2025-04-24T01:15:33.153483Z",
     "shell.execute_reply.started": "2025-04-24T01:15:33.108015Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(env, actor, n_games=1, t_max=1600):\n",
    "    '''\n",
    "    Запускает n_games эпизодов и возвращает массив наград.\n",
    "    \n",
    "    Возвращает\n",
    "    -------\n",
    "    rewards: np.array\n",
    "        Массив наград\n",
    "    '''\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        R = 0\n",
    "\n",
    "        for _ in range(t_max):\n",
    "            \n",
    "            action = actor.get_action(torch.tensor(s).to(DEVICE))\n",
    "\n",
    "            assert (action.max() <= 1).all() and  (action.min() >= -1).all()\n",
    "\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            R += r\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.180382Z",
     "start_time": "2025-04-24T08:13:45.839786Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:33.906903Z",
     "iopub.status.busy": "2025-04-24T01:15:33.906195Z",
     "iopub.status.idle": "2025-04-24T01:15:35.441276Z",
     "shell.execute_reply": "2025-04-24T01:15:35.439642Z",
     "shell.execute_reply.started": "2025-04-24T01:15:33.906861Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semyon/.local/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/semyon/ML/RL/ContinuousControl/videos_test folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/semyon/ML/RL/ContinuousControl/videos_test/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/semyon/ML/RL/ContinuousControl/videos_test/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/semyon/ML/RL/ContinuousControl/videos_test/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_test\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.184346Z",
     "start_time": "2025-04-24T08:13:47.181920Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:36.173059Z",
     "iopub.status.busy": "2025-04-24T01:15:36.172218Z",
     "iopub.status.idle": "2025-04-24T01:15:36.240213Z",
     "shell.execute_reply": "2025-04-24T01:15:36.238729Z",
     "shell.execute_reply.started": "2025-04-24T01:15:36.173016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_test/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_test').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Буфер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, что и в DQN. Вы можете просто скопировать код из вашего задания по DQN.\n",
    "\n",
    "#### Напомним интерфейс:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)`  &mdash; сохраняет кортеж (s,a,r,s',done) в буфер\n",
    "* `exp_replay.sample(batch_size)` &mdash; возвращает наблюдения, действия, награды, следующие наблюдения и is_done для `batch_size` случайных сэмплов.\n",
    "* `len(exp_replay)` &mdash; возвращает количество элементов, хранящихся в буфере на данный момент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.196890Z",
     "start_time": "2025-04-24T08:13:47.184939Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:42.800849Z",
     "iopub.status.busy": "2025-04-24T01:15:42.799925Z",
     "iopub.status.idle": "2025-04-24T01:15:42.845602Z",
     "shell.execute_reply": "2025-04-24T01:15:42.844035Z",
     "shell.execute_reply.started": "2025-04-24T01:15:42.800806Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, alpha=0.6):\n",
    "        \"\"\"\n",
    "        Создаёт буфер реплеев.\n",
    "        Параметры\n",
    "        ----------\n",
    "        size: int\n",
    "            Максимальное количество хранящихся единовременно переходов. При переполнении буфера старые\n",
    "            записи удаляются.\n",
    "\n",
    "        Замечание: для данного задания можно выбрать любую структуру данных.\n",
    "              Если Вам достаточно простого решения, то можно просто хранить список кортежей (s, a, r, s')\n",
    "              в self._storage. Однако, можно найти более быстрые и/или эффективные по памяти способы\n",
    "              реализации самого хранения переходов.\n",
    "        \"\"\"\n",
    "        #self._storage = []\n",
    "        self._storage = deque(maxlen=size)\n",
    "        self._maxsize = size\n",
    "        self._priorities = deque(maxlen=size)\n",
    "        self._alpha = alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add_random(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Убедитесь, что _storage не превзойдёт по размерам _maxsize.\n",
    "        Убедитесь, что FIFO правило выполняется: старейшие прецеденты должны удаляться раньше всех.\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        self._storage.append(data)\n",
    "\n",
    "    def sample_random(self, batch_size):\n",
    "        \"\"\"Сэмплирование батча переходов.\n",
    "        Параметры\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Сколько переходов сэмплировать.\n",
    "        Возвращает\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            батч наблюдений (состояний)\n",
    "        act_batch: np.array\n",
    "            батч действий, выполненных на основе obs_batch\n",
    "        rew_batch: np.array\n",
    "            награды, полученные в качестве результата выполнения act_batch\n",
    "        next_obs_batch: np.array\n",
    "            следующие наблюдения (состояния), полученные после выполнения act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1, если выполнение act_batch[i] повлекло\n",
    "            окончание эпизода и 0 иначе.\n",
    "        \"\"\"\n",
    "        #storage = self._storage\n",
    "\n",
    "        #< случайно сгенерировать batch_size индексов сэмплов в буфере >\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "\n",
    "        # собрать <s,a,r,s',done> для каждого индекса\n",
    "        states, actions, rewards, next_states, is_dones = [], [], [], [], []\n",
    "        for idx in idxes:\n",
    "            state, action, reward, next_state, is_done = self._storage[idx]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            is_dones.append(is_done)\n",
    "\n",
    "        # < states > , < actions >, < rewards >,  < next_states >, < is_done >\n",
    "        return (np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(is_dones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.271903Z",
     "start_time": "2025-04-24T08:13:47.197662Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:44.563368Z",
     "iopub.status.busy": "2025-04-24T01:15:44.562684Z",
     "iopub.status.idle": "2025-04-24T01:15:44.856966Z",
     "shell.execute_reply": "2025-04-24T01:15:44.855180Z",
     "shell.execute_reply.started": "2025-04-24T01:15:44.563324Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add_random(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"Размер буфера должен быть равен 10, потому что это максимальная вместимость\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для записи в буффер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.274946Z",
     "start_time": "2025-04-24T08:13:47.272633Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:46.034635Z",
     "iopub.status.busy": "2025-04-24T01:15:46.033751Z",
     "iopub.status.idle": "2025-04-24T01:15:46.052089Z",
     "shell.execute_reply": "2025-04-24T01:15:46.051347Z",
     "shell.execute_reply.started": "2025-04-24T01:15:46.034590Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Играется в среде ровно n шагов и записывается каждая пятёрка (s, a, r, s', done) в буффер.\n",
    "\n",
    "    Возвращает\n",
    "    -------\n",
    "        sum_rewards: float\n",
    "            суммарную награду за n шагов.\n",
    "        s: float\n",
    "            состояние, в котором осталась среда.\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        \n",
    "        a = agent.get_action(torch.tensor(s).to(DEVICE))\n",
    "        \n",
    "        ns, r, terminated, truncated, _ = env.step(a)\n",
    "        \n",
    "        exp_replay.add_random(s, a, r, ns, terminated)\n",
    "        \n",
    "        s = env.reset()[0] if terminated or truncated else ns\n",
    "        \n",
    "        sum_rewards += r        \n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:54:30.667197Z",
     "start_time": "2025-04-23T20:54:30.663277Z"
    }
   },
   "source": [
    "# Критик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте начнём с модели критика &mdash; она одинаковая для **TD3** и **SAC**. С одной стороны, он будет приближать оптимальную функцию $Q^*(s, a)$, а с другой &mdash; оценивать текущего актора $\\pi$, то есть рассматриваться как \n",
    "$Q^{\\pi}(s, a)$. Этот критик принимает на вход как состояние $s$, так и действие $a$, а на выходе выдаёт скалярное значение.\n",
    "\n",
    "Важно: если модель возвращает скаляр на выходе, хорошей практикой является применение .squeeze(), чтобы избежать неожиданного broadcast-а, поскольку тензор формы [batch_size, 1] может автоматически расширяться при операциях с другими тензорами.\n",
    "\n",
    "Рекомендуемая архитектура &mdash; полносвязная нейросеть (MLP) с тремя слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(1 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.907009Z",
     "start_time": "2025-04-24T08:13:47.895580Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:47.267769Z",
     "iopub.status.busy": "2025-04-24T01:15:47.266895Z",
     "iopub.status.idle": "2025-04-24T01:15:47.330991Z",
     "shell.execute_reply": "2025-04-24T01:15:47.329354Z",
     "shell.execute_reply.started": "2025-04-24T01:15:47.267721Z"
    }
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()  \n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.action_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        input = torch.cat((states, actions), dim=-1)\n",
    "        output = self.model(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_qvalues(self, states, actions):\n",
    "        '''\n",
    "        Возвращает qvalues\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "            \n",
    "        Возвращает\n",
    "        -------\n",
    "        qvalues: torch.tensor [batch_size]\n",
    "        '''\n",
    "        qvalues = self.forward(states, actions).squeeze()\n",
    "\n",
    "        assert len(qvalues.shape) == 1 and qvalues.shape[0] == states.shape[0]\n",
    "        \n",
    "        return qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:48.310526Z",
     "start_time": "2025-04-24T08:13:48.301042Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:15:48.081570Z",
     "iopub.status.busy": "2025-04-24T01:15:48.080863Z",
     "iopub.status.idle": "2025-04-24T01:15:48.109147Z",
     "shell.execute_reply": "2025-04-24T01:15:48.107729Z",
     "shell.execute_reply.started": "2025-04-24T01:15:48.081519Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"TD3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим политику, или актора $\\pi$. Необходимо смоделировать детерминированную политику. То есть, модель должна возвращать `action_dim` чисел в диапазоне $[-1, 1]$. К сожалению, детерминированные политики могут вызывать проблемы со стабильностью и исследованием, поэтому нам потребуется реализовать три \"режима\" работы этой политики:\n",
    "\n",
    "1. Первый режим &mdash; **жадный** &mdash; это просто прямой проход через сеть. Он будет использоваться для обучения актора.\n",
    "2. Второй режим &mdash; **режим исследования** &mdash; когда нужно добавить шум (например, гауссовский), чтобы собирать более разнообразные данные.\n",
    "3. Третий режим &mdash; **\"обрезанный шум\"** &mdash; используется при расчёте целевого значения для критика. Здесь мы хотим добавить шум к выходу актора, но не слишком сильный, поэтому используем обрезанный шум:\n",
    "$$\\pi_{\\theta}(s) + \\varepsilon, \\quad \\varepsilon = \\operatorname{clip}(\\epsilon, -0.5, 0.5), \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$$\n",
    "\n",
    "Рекомендуемая архитектура &mdash; полносвязная нейронная сеть (MLP) с тремя слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(2 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:49.201849Z",
     "start_time": "2025-04-24T08:13:49.191425Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:20:06.545534Z",
     "iopub.status.busy": "2025-04-24T01:20:06.544435Z",
     "iopub.status.idle": "2025-04-24T01:20:06.597176Z",
     "shell.execute_reply": "2025-04-24T01:20:06.595678Z",
     "shell.execute_reply.started": "2025-04-24T01:20:06.545489Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TD3_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, device, hidden_dim=256):\n",
    "        super().__init__() \n",
    "        self.device = device\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim // 2, self.action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        output = self.model(state)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_best_action(self, states):\n",
    "        '''\n",
    "        Используется для оптимизации актора.\n",
    "        Требуется, чтобы действия были дифференцируемыми по параметрам модели.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "            \n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "        '''\n",
    "        actions = self.forward(states)\n",
    "\n",
    "        return actions\n",
    "        \n",
    "    def get_action(self, states, std_noise=0.1):\n",
    "        '''\n",
    "        Используется для взаимодействия с окружающей средой и сбора данных.\n",
    "        Поэтому к действиям необходимо добавлять шум.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        states: np.array [batch_size x features]\n",
    "        std_noise: float\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: np.array [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут не нужны, так как используется только для взаимодействия\n",
    "        with torch.no_grad():\n",
    "            actions = self.forward(states)\n",
    "            noise = torch.randn_like(actions) * std_noise\n",
    "            actions = torch.clip(actions + noise, -1, 1)\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions.data.cpu().numpy()\n",
    "\n",
    "    \n",
    "    def get_target_action(self, states, std_noise=0.2, clip_eta=0.5):\n",
    "        '''\n",
    "        Используется для генерации целевых значений при обучении критика.\n",
    "        Возвращает действия с добавленным \"обрезанным шумом\".\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        std_noise: float\n",
    "        clip_eta: float\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут также не нужны, используется только для генерации цели\n",
    "        with torch.no_grad():\n",
    "            actions = self.forward(states)\n",
    "            noise = torch.randn_like(actions) * std_noise\n",
    "            clipped_noise = torch.clip(noise, -clip_eta, clip_eta)\n",
    "            actions = torch.clip(actions + clipped_noise, -1, 1)\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:50.166671Z",
     "start_time": "2025-04-24T08:13:50.004540Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:20:07.571472Z",
     "iopub.status.busy": "2025-04-24T01:20:07.570698Z",
     "iopub.status.idle": "2025-04-24T01:20:08.180459Z",
     "shell.execute_reply": "2025-04-24T01:20:08.178584Z",
     "shell.execute_reply.started": "2025-04-24T01:20:07.571430Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(2000)\n",
    "actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "\n",
    "state, _ = env.reset()\n",
    "play_and_record(state, actor, env, exp_replay, n_steps=1000)\n",
    "\n",
    "assert len(exp_replay) == 1000, \\\n",
    "    \"play_and_record должен был добавить ровно 1000 шагов, но вместо этого добавил %i\" % len(exp_replay)\n",
    "\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(10)\n",
    "\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + (state_dim,), \\\n",
    "        \"Батчи наблюдений и следующих состояний должны иметь форму (10, %d)\" % state_dim\n",
    "\n",
    "    assert act_batch.shape == (10, action_dim), \\\n",
    "        \"Батч действий должен иметь форму (10, 8), но вместо этого: %s\" % str(act_batch.shape)\n",
    "\n",
    "    assert reward_batch.shape == (10,), \\\n",
    "        \"Батч наград должен иметь форму (10,), но вместо этого: %s\" % str(reward_batch.shape)\n",
    "\n",
    "    assert is_done_batch.shape == (10,), \\\n",
    "        \"Батч is_done должен иметь форму (10,), но вместо этого: %s\" % str(is_done_batch.shape)\n",
    "\n",
    "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
    "        \"is_done должен быть строго True или False (или 1/0)\"\n",
    "\n",
    "print(\"Отлично!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:50.990771Z",
     "start_time": "2025-04-24T08:13:50.988283Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:20:09.658727Z",
     "iopub.status.busy": "2025-04-24T01:20:09.657921Z",
     "iopub.status.idle": "2025-04-24T01:20:09.676124Z",
     "shell.execute_reply": "2025-04-24T01:20:09.674674Z",
     "shell.execute_reply.started": "2025-04-24T01:20:09.658682Z"
    }
   },
   "outputs": [],
   "source": [
    "max_buffer_size = 5 * 10**5\n",
    "exp_replay = ReplayBuffer(max_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:51.367535Z",
     "start_time": "2025-04-24T08:13:51.358427Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:20:53.946750Z",
     "iopub.status.busy": "2025-04-24T01:20:53.945851Z",
     "iopub.status.idle": "2025-04-24T01:20:53.988490Z",
     "shell.execute_reply": "2025-04-24T01:20:53.986954Z",
     "shell.execute_reply.started": "2025-04-24T01:20:53.946706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы стабилизировать обучение, нам понадобятся целевые сети — медленно обновляемые копии наших моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:51.835845Z",
     "start_time": "2025-04-24T08:13:51.822962Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:21:18.755552Z",
     "iopub.status.busy": "2025-04-24T01:21:18.754627Z",
     "iopub.status.idle": "2025-04-24T01:21:18.787435Z",
     "shell.execute_reply": "2025-04-24T01:21:18.786Z",
     "shell.execute_reply.started": "2025-04-24T01:21:18.755504Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "target_critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "target_critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задачах с непрерывным управлением целевые сети обычно обновляются с использованием экспоненциального сглаживания:\n",
    "$$\\theta^{-} \\leftarrow \\tau \\theta + (1 - \\tau) \\theta^{-},$$\n",
    "где $\\theta^{-}$ &mdash; веса дополнительной сети, $\\theta$ &mdash; текущие параметры модели, а $\\tau$ &mdash; гиперпараметр (коэффициент обновления)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:52.256363Z",
     "start_time": "2025-04-24T08:13:52.250709Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:21:19.861923Z",
     "iopub.status.busy": "2025-04-24T01:21:19.861131Z",
     "iopub.status.idle": "2025-04-24T01:21:19.887168Z",
     "shell.execute_reply": "2025-04-24T01:21:19.885846Z",
     "shell.execute_reply.started": "2025-04-24T01:21:19.861879Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_target_networks(model, target_model, tau=0.005):\n",
    "    for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, у нас будет три отдельных процедуры оптимизации для обучения трёх моделей, так что давайте поприветствуем наших трёх Адамов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:52.658128Z",
     "start_time": "2025-04-24T08:13:52.650846Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:21:21.767302Z",
     "iopub.status.busy": "2025-04-24T01:21:21.766522Z",
     "iopub.status.idle": "2025-04-24T01:21:21.793875Z",
     "shell.execute_reply": "2025-04-24T01:21:21.792348Z",
     "shell.execute_reply.started": "2025-04-24T01:21:21.767259Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:53.063674Z",
     "start_time": "2025-04-24T08:13:53.057164Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:21:23.653696Z",
     "iopub.status.busy": "2025-04-24T01:21:23.652967Z",
     "iopub.status.idle": "2025-04-24T01:21:23.674508Z",
     "shell.execute_reply": "2025-04-24T01:21:23.673529Z",
     "shell.execute_reply.started": "2025-04-24T01:21:23.653655Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize(name, model, optimizer, loss, n_iterations, max_grad_norm=10):\n",
    "    '''\n",
    "    Выполняет один шаг оптимизации, ограничивает норму градиента значением max_grad_norm \n",
    "    и логирует всё в TensorBoard.\n",
    "    '''\n",
    "    loss = loss.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    env.writer.add_scalar(name, loss.item(), n_iterations)\n",
    "    env.writer.add_scalar(name + \"_grad_norm\", grad_norm.item(), n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление целевого значения для критика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для одного сэмплированного перехода $(s, a, r, s')$ целевое значение имеет вид:\n",
    "$$y(s, a) = r + \\gamma V(s').$$\n",
    "Как вычисляется $V(s')$? Формально, оценка Монте-Карло выглядит просто:\n",
    "$$V(s') \\approx Q(s', a'),$$\n",
    "где $a'$ &mdash; это сэмпл из текущей политики $\\pi(a' \\mid s')$.\n",
    "\n",
    "Однако наш актор $\\pi$ обучается выбирать такие действия $a'$, где критик выдаёт наибольшие значения, что может приводить к завышенным оценкам. Чтобы избежать этого, мы используем несколько приёмов:\n",
    "\n",
    "1. Используем двух критиков (берём минимум из их значений):\n",
    "$$V(s') = \\min_{i = 1,2} Q^{-}_i(s', a'),$$\n",
    "где $a'$ &mdash; это сэмпл из целевой политики: $\\pi^{-}(a' \\mid s')$.\n",
    "2. Для вычисления $a'$ используется режим с обрезанным шумом, чтобы не позволить политике эксплуатировать узкие пики в $Q$-функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.138205Z",
     "start_time": "2025-04-24T01:58:06.138199Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(0.5 балл)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:54.158713Z",
     "start_time": "2025-04-24T08:13:54.154703Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:21:25.009449Z",
     "iopub.status.busy": "2025-04-24T01:21:25.008710Z",
     "iopub.status.idle": "2025-04-24T01:21:25.053197Z",
     "shell.execute_reply": "2025-04-24T01:21:25.051902Z",
     "shell.execute_reply.started": "2025-04-24T01:21:25.009407Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_critic_target(rewards, next_states, is_done, target_actor, target_critic1, target_critic2,\n",
    "                          gamma=0.99):\n",
    "    '''\n",
    "    Подсчет loss для критика.\n",
    "    \n",
    "    Параметры\n",
    "        ----------\n",
    "        rewards: torch.tensor [batch_size]\n",
    "        next_states: torch.tensor [batch_size x features]\n",
    "        is_done: torch.tensor [batch_size]\n",
    "        gamma: float\n",
    "        \n",
    "    Возвращает\n",
    "        -------\n",
    "        critic_target: torch.tensor [batch_size]\n",
    "    '''\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    with torch.no_grad():\n",
    "        actions = target_actor.get_target_action(next_states)\n",
    "        q1 = target_critic1.get_qvalues(next_states, actions)\n",
    "        q2 = target_critic2.get_qvalues(next_states, actions)\n",
    "\n",
    "        state_values = torch.min(q1, q2)\n",
    "\n",
    "        critic_target = rewards + gamma * is_not_done * state_values\n",
    "    \n",
    "    return critic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения актора мы хотим просто максимизировать:\n",
    "$$\\mathbb{E}_{a \\sim \\pi(a \\mid s)} Q(s, a) \\to \\max_{\\pi}.$$\n",
    "Так как политика детерминированная, математическое ожидание сводится к:\n",
    "$$Q(s, \\pi(s)) \\to \\max_{\\pi}.$$\n",
    "\n",
    "**Замечание:**  \n",
    "Мы будем использовать `critic1` в качестве функции $Q$, которую актор будет пытаться \"эксплуатировать\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(0.5 балл)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:54.997518Z",
     "start_time": "2025-04-24T08:13:54.991477Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:21:27.437504Z",
     "iopub.status.busy": "2025-04-24T01:21:27.436726Z",
     "iopub.status.idle": "2025-04-24T01:21:27.514020Z",
     "shell.execute_reply": "2025-04-24T01:21:27.512661Z",
     "shell.execute_reply.started": "2025-04-24T01:21:27.437463Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(states, actor, critic1):\n",
    "    '''\n",
    "    Подсчет loss для актора.\n",
    "    \n",
    "    \n",
    "    Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        \n",
    "    Возвращает\n",
    "        -------\n",
    "        actor_loss: torch.tensor [batch_size]\n",
    "    '''\n",
    "    actions = actor.get_best_action(states)\n",
    "    qvalues = critic1.get_qvalues(states, actions)\n",
    "    actor_loss = -qvalues.mean()\n",
    "    \n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пайплайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель &mdash; достичь в среднем хотя бы **300 награды**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:55.748252Z",
     "start_time": "2025-04-24T08:13:55.741014Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:21:29.375084Z",
     "iopub.status.busy": "2025-04-24T01:21:29.374369Z",
     "iopub.status.idle": "2025-04-24T01:21:29.409673Z",
     "shell.execute_reply": "2025-04-24T01:21:29.407869Z",
     "shell.execute_reply.started": "2025-04-24T01:21:29.375042Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 11 # Иногда может сильно не повести\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:57.684751Z",
     "start_time": "2025-04-24T08:13:56.642708Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:21:30.038955Z",
     "iopub.status.busy": "2025-04-24T01:21:30.038123Z",
     "iopub.status.idle": "2025-04-24T01:21:50.690461Z",
     "shell.execute_reply": "2025-04-24T01:21:50.688903Z",
     "shell.execute_reply.started": "2025-04-24T01:21:30.038904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 69504), started 0:12:28 ago. (Use '!kill 69504' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-62233d0679a4d0bf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-62233d0679a4d0bf\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:14:02.418310Z",
     "start_time": "2025-04-24T08:14:02.404628Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:23:03.918049Z",
     "iopub.status.busy": "2025-04-24T01:23:03.917169Z",
     "iopub.status.idle": "2025-04-24T01:23:03.961330Z",
     "shell.execute_reply": "2025-04-24T01:23:03.959986Z",
     "shell.execute_reply.started": "2025-04-24T01:23:03.918006Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2,\n",
    "            max_grad_norm=10, n_iter_max=500000, timesteps_per_epoch=1, start_timesteps=10000,\n",
    "            batch_size=128, policy_update_freq=1, gamma=0.99, tau=0.005):\n",
    "    \n",
    "    interaction_state, _ = env.reset()\n",
    "    random_actor = RandomActor()\n",
    "    \n",
    "    for n_iterations in trange(0, n_iter_max, timesteps_per_epoch):\n",
    "        \n",
    "        if len(exp_replay) < start_timesteps:\n",
    "            _, interaction_state = play_and_record(interaction_state, random_actor, env,\n",
    "                                                   exp_replay, timesteps_per_epoch)\n",
    "            continue\n",
    "\n",
    "        _, interaction_state = play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        states, actions, rewards, next_states, is_done = exp_replay.sample_random(batch_size)\n",
    "\n",
    "        states = torch.tensor(states, device=DEVICE, dtype=torch.float)\n",
    "        actions = torch.tensor(actions, device=DEVICE, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float)\n",
    "        next_states = torch.tensor(next_states, device=DEVICE, dtype=torch.float)\n",
    "        is_done = torch.tensor(is_done.astype('float32'), device=DEVICE, dtype=torch.float)\n",
    "        \n",
    "        critic_target = compute_critic_target(rewards, next_states, is_done, target_actor, target_critic1, target_critic2, gamma)\n",
    "\n",
    "        critic1_predict = critic1.get_qvalues(states, actions)\n",
    "        critic1_loss = torch.nn.functional.mse_loss(critic1_predict, critic_target) \n",
    "        optimize(\"critic1\", critic1, opt_critic1, critic1_loss, max_grad_norm)\n",
    "        \n",
    "        critic2_predict = critic2.get_qvalues(states, actions)\n",
    "        critic2_loss = torch.nn.functional.mse_loss(critic2_predict, critic_target) \n",
    "        optimize(\"critic2\", critic2, opt_critic2, critic2_loss, max_grad_norm)\n",
    "\n",
    "        if n_iterations % policy_update_freq == 0:\n",
    "            actor_loss = compute_actor_loss(states, actor, critic1)\n",
    "            optimize(\"actor\", actor, opt_actor, actor_loss, max_grad_norm)\n",
    "\n",
    "            update_target_networks(critic1, target_critic1, tau)\n",
    "            update_target_networks(critic2, target_critic2, tau)\n",
    "            update_target_networks(actor, target_actor, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:15.155234Z",
     "start_time": "2025-04-24T08:14:03.190411Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:23:04.713632Z",
     "iopub.status.busy": "2025-04-24T01:23:04.712930Z",
     "iopub.status.idle": "2025-04-24T01:23:07.153645Z",
     "shell.execute_reply": "2025-04-24T01:23:07.151419Z",
     "shell.execute_reply.started": "2025-04-24T01:23:04.713590Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6629e5b6c99f417791803f1039db4bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_TD3\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_replay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_critic1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_critic2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 17\u001b[0m, in \u001b[0;36mrun_TD3\u001b[0;34m(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2, max_grad_norm, n_iter_max, timesteps_per_epoch, start_timesteps, batch_size, policy_update_freq, gamma, tau)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m _, interaction_state \u001b[38;5;241m=\u001b[39m play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n\u001b[0;32m---> 17\u001b[0m states, actions, rewards, next_states, is_done \u001b[38;5;241m=\u001b[39m \u001b[43mexp_replay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_random\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(states, device\u001b[38;5;241m=\u001b[39mDEVICE, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     20\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, device\u001b[38;5;241m=\u001b[39mDEVICE, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "Cell \u001b[0;32mIn[36], line 64\u001b[0m, in \u001b[0;36mReplayBuffer.sample_random\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m idxes:\n\u001b[1;32m     63\u001b[0m     state, action, reward, next_state, is_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage[idx]\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mstates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[1;32m     66\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.141979Z",
     "start_time": "2025-04-24T01:58:06.141974Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(1.5 балл)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:19.633149Z",
     "start_time": "2025-04-24T08:42:17.757596Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:23:11.071450Z",
     "iopub.status.busy": "2025-04-24T01:23:11.070685Z",
     "iopub.status.idle": "2025-04-24T01:23:12.262262Z",
     "shell.execute_reply": "2025-04-24T01:23:12.260347Z",
     "shell.execute_reply.started": "2025-04-24T01:23:11.071406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша нагрда: 302.96905645288535\n",
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "sessions = evaluate(env, actor, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Ваша нагрда: {score}\")\n",
    "\n",
    "assert score >= 300, \"Нужно больше учить?\"\n",
    "print(\"Отлично!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:23.976927Z",
     "start_time": "2025-04-24T08:42:22.393814Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:23:15.111602Z",
     "iopub.status.busy": "2025-04-24T01:23:15.110877Z",
     "iopub.status.idle": "2025-04-24T01:23:15.872191Z",
     "shell.execute_reply": "2025-04-24T01:23:15.870490Z",
     "shell.execute_reply.started": "2025-04-24T01:23:15.111558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/semyon/ML/RL/ContinuousControl/videos_TD3/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/semyon/ML/RL/ContinuousControl/videos_TD3/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/semyon/ML/RL/ContinuousControl/videos_TD3/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_TD3\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:23.980665Z",
     "start_time": "2025-04-24T08:42:23.978191Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:23:17.207905Z",
     "iopub.status.busy": "2025-04-24T01:23:17.207203Z",
     "iopub.status.idle": "2025-04-24T01:23:17.231558Z",
     "shell.execute_reply": "2025-04-24T01:23:17.230124Z",
     "shell.execute_reply.started": "2025-04-24T01:23:17.207862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_TD3/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_TD3').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), \"actor_td3.pth\")\n",
    "torch.save(critic1.state_dict(), \"critic1_td3.pth\")\n",
    "torch.save(critic2.state_dict(), \"critic2_td3.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:31.821856Z",
     "start_time": "2025-04-24T08:59:31.818368Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:23:22.256273Z",
     "iopub.status.busy": "2025-04-24T01:23:22.255449Z",
     "iopub.status.idle": "2025-04-24T01:23:22.285366Z",
     "shell.execute_reply": "2025-04-24T01:23:22.283878Z",
     "shell.execute_reply.started": "2025-04-24T01:23:22.256231Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"SAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно смоделировать гауссовскую политику. Это означает, что распределение политики &mdash; это многомерное нормальное распределение с диагональной ковариационной матрицей. Должны предсказываться среднее и ковариация, при этом важно гарантировать, что ковариация остаётся неотрицательной. Пусть $f_{\\theta}(s)$ &mdash; выход головы ковариации, преобразуем выход в диапазон $[-1, 1]$ с помощью `tanh`, затем спроецируем результат в интервал $[m, M]$, где $m = -20$, $M = 2$, и применим экспоненту. Это обеспечит адекватный диапазон ковариации:\n",
    "$$\\sigma(s) = \\exp^{m + 0.5(M - m)(\\tanh(f_{\\theta}(s)) + 1)}.$$\n",
    "\n",
    "Гауссовское распределение не ограничено, но нужно, чтобы действия лежали в диапазоне $[-1, 1]$. Для этого:\n",
    "1. Моделируем неограниченное распределение:  \n",
    "   $\\mathcal{N}(\\mu_{\\theta}(s), \\sigma_{\\theta}(s)^2I)$\n",
    "2. Затем каждую выборку $u$ из этого распределения преобразуем с помощью $\\tanh$:\n",
    "$$u \\sim \\mathcal{N}(\\mu, \\sigma^2I)$$\n",
    "$$a = \\tanh(u)$$\n",
    "\n",
    "**Важно:** После применения $\\tanh$ необходимо использовать формулу замены переменных при вычислении логарифма плотности вероятности:\n",
    "$$\\log p(a \\mid \\mu, \\sigma) = \\log p(u \\mid \\mu, \\sigma) - \\sum_{i = 1}^D \\log \\left(1 - \\tanh^2(u_i)\\right),$$\n",
    "где $D$ &mdash; размерность действия (`action_dim`). На практике желательно добавить небольшое значение (например, $1e{-6}$) внутрь логарифма для числовой устойчивости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(2 балл)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:36.275645Z",
     "start_time": "2025-04-24T08:59:36.272221Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:30.738130Z",
     "iopub.status.busy": "2025-04-24T01:24:30.737182Z",
     "iopub.status.idle": "2025-04-24T01:24:30.789259Z",
     "shell.execute_reply": "2025-04-24T01:24:30.787821Z",
     "shell.execute_reply.started": "2025-04-24T01:24:30.738086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAC_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, device, hidden_dim=256, m=-20, M=2):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.m = m\n",
    "        self.M = M\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.mean_head = nn.Linear(self.hidden_dim // 2, self.action_dim)\n",
    "        self.variance_head = nn.Linear(self.hidden_dim // 2, self.action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        features = self.model(state)\n",
    "        mean = self.mean_head(features)\n",
    "    \n",
    "        cov = self.variance_head(features)\n",
    "        cov = 0.5 * (torch.tanh(cov) + 1)\n",
    "        cov = torch.exp(self.m + (self.M - self.m) * cov)\n",
    "\n",
    "        return mean, cov\n",
    "    \n",
    "    def apply(self, states):\n",
    "        '''\n",
    "        Сэмплирует действия.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "            states: torch.tensor [batch_size x features]\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "            actions: torch.tensor [batch_size x actions_dim]\n",
    "            log_prob: torch.tensor [batch_size]\n",
    "        '''\n",
    "        mean, cov = self.forward(states)\n",
    "\n",
    "        distribution = Normal(mean, cov)\n",
    "        u = distribution.rsample()\n",
    "        actions = torch.tanh(u)\n",
    "\n",
    "        log_prob = distribution.log_prob(u) - torch.log(1 - actions**2 + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1)\n",
    "        \n",
    "        return actions, log_prob  \n",
    "\n",
    "    def get_action(self, states):\n",
    "        '''\n",
    "        Используется для взаимодействия с окружающей средой и сбора данных.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        states: np.array [batch_size x features]\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: np.array [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут не нужны, так как используется только для взаимодействия\n",
    "        with torch.no_grad():\n",
    "            states = torch.tensor(states).to(self.device)\n",
    "            actions, log_prob = self.apply(states)\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:39.079174Z",
     "start_time": "2025-04-24T08:59:38.889928Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:31.661260Z",
     "iopub.status.busy": "2025-04-24T01:24:31.660490Z",
     "iopub.status.idle": "2025-04-24T01:24:32.705036Z",
     "shell.execute_reply": "2025-04-24T01:24:32.703427Z",
     "shell.execute_reply.started": "2025-04-24T01:24:31.661218Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68400/2331244515.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(2000)\n",
    "actor = SAC_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "\n",
    "state, _ = env.reset()\n",
    "play_and_record(state, actor, env, exp_replay, n_steps=1000)\n",
    "\n",
    "assert len(exp_replay) == 1000, \\\n",
    "    \"play_and_record должен был добавить ровно 1000 шагов, но вместо этого добавил %i\" % len(exp_replay)\n",
    "\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(10)\n",
    "\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + (state_dim,), \\\n",
    "        \"Батчи наблюдений и следующих состояний должны иметь форму (10, %d)\" % state_dim\n",
    "\n",
    "    assert act_batch.shape == (10, action_dim), \\\n",
    "        \"Батч действий должен иметь форму (10, 8), но вместо этого: %s\" % str(act_batch.shape)\n",
    "\n",
    "    assert reward_batch.shape == (10,), \\\n",
    "        \"Батч наград должен иметь форму (10,), но вместо этого: %s\" % str(reward_batch.shape)\n",
    "\n",
    "    assert is_done_batch.shape == (10,), \\\n",
    "        \"Батч is_done должен иметь форму (10,), но вместо этого: %s\" % str(is_done_batch.shape)\n",
    "\n",
    "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
    "        \"is_done должен быть строго True или False (или 1/0)\"\n",
    "\n",
    "print(\"Отлично!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:41.091591Z",
     "start_time": "2025-04-24T08:59:41.089590Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:33.395399Z",
     "iopub.status.busy": "2025-04-24T01:24:33.394680Z",
     "iopub.status.idle": "2025-04-24T01:24:33.420304Z",
     "shell.execute_reply": "2025-04-24T01:24:33.418777Z",
     "shell.execute_reply.started": "2025-04-24T01:24:33.395357Z"
    }
   },
   "outputs": [],
   "source": [
    "max_buffer_size = 10**5\n",
    "exp_replay = ReplayBuffer(max_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:41.582761Z",
     "start_time": "2025-04-24T08:59:41.579812Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:40.006873Z",
     "iopub.status.busy": "2025-04-24T01:24:40.006126Z",
     "iopub.status.idle": "2025-04-24T01:24:40.034494Z",
     "shell.execute_reply": "2025-04-24T01:24:40.032976Z",
     "shell.execute_reply.started": "2025-04-24T01:24:40.006829Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor = SAC_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:42.130567Z",
     "start_time": "2025-04-24T08:59:42.127553Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:40.624296Z",
     "iopub.status.busy": "2025-04-24T01:24:40.623459Z",
     "iopub.status.idle": "2025-04-24T01:24:40.644718Z",
     "shell.execute_reply": "2025-04-24T01:24:40.643303Z",
     "shell.execute_reply.started": "2025-04-24T01:24:40.624253Z"
    }
   },
   "outputs": [],
   "source": [
    "target_critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "target_critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:50.695410Z",
     "start_time": "2025-04-24T08:59:50.686991Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:41.557378Z",
     "iopub.status.busy": "2025-04-24T01:24:41.556538Z",
     "iopub.status.idle": "2025-04-24T01:24:41.575446Z",
     "shell.execute_reply": "2025-04-24T01:24:41.574207Z",
     "shell.execute_reply.started": "2025-04-24T01:24:41.557334Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление целевого значения для критика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия от **TD3**:\n",
    "\n",
    "1. Используем двух критиков (берём минимум из их значений):\n",
    "$$V(s') = \\min_{i = 1, 2} Q^{-}_i(s', a'),$$\n",
    "где $a'$ &mdash; это сэмпл из целевой политики $\\pi(a' \\mid s')$;\n",
    "2. Добавляется энтропийный бонус:\n",
    "$$V(s') = \\min_{i = 1, 2} Q^{-}_i(s', a') - \\alpha \\log \\pi(a' \\mid s').$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.145091Z",
     "start_time": "2025-04-24T01:58:06.145086Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(0.5 балл)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:58.722072Z",
     "start_time": "2025-04-24T08:59:58.712927Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:44.020840Z",
     "iopub.status.busy": "2025-04-24T01:24:44.019329Z",
     "iopub.status.idle": "2025-04-24T01:24:44.042512Z",
     "shell.execute_reply": "2025-04-24T01:24:44.041225Z",
     "shell.execute_reply.started": "2025-04-24T01:24:44.020777Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_critic_target_sac(rewards, next_states, is_done, actor, target_critic1, target_critic2,\n",
    "                          gamma=0.99, alpha=0.4):\n",
    "    '''\n",
    "    Подсчет loss для критика.\n",
    "    \n",
    "    Параметры\n",
    "        ----------\n",
    "        rewards: torch.tensor [batch_size]\n",
    "        next_states: torch.tensor [batch_size x features]\n",
    "        is_done: torch.tensor [batch_size]\n",
    "        gamma: float\n",
    "        alpha: float\n",
    "        \n",
    "    Возвращает\n",
    "        -------\n",
    "        critic_target: torch.tensor [batch_size]\n",
    "    '''\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    with torch.no_grad():\n",
    "        actions, log_prob = actor.apply(next_states)\n",
    "        q1 = target_critic1.get_qvalues(next_states, actions)\n",
    "        q2 = target_critic2.get_qvalues(next_states, actions)\n",
    "\n",
    "        state_values = torch.min(q1, q2) - alpha * log_prob\n",
    "\n",
    "        critic_target = rewards + gamma * is_not_done * state_values\n",
    "    \n",
    "    return critic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия от **TD3**:\n",
    "\n",
    "Добавляется регуляризатор энтропии, чтобы стимулировать стохастичность политики:\n",
    "$$\\mathbb{E}_{a \\sim \\pi(a \\mid s)} Q(s, a) - \\alpha \\log \\pi(a \\mid s) \\to \\max_{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(0.5 балл)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:00:01.896966Z",
     "start_time": "2025-04-24T09:00:01.892117Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:44.751023Z",
     "iopub.status.busy": "2025-04-24T01:24:44.750283Z",
     "iopub.status.idle": "2025-04-24T01:24:44.767721Z",
     "shell.execute_reply": "2025-04-24T01:24:44.766637Z",
     "shell.execute_reply.started": "2025-04-24T01:24:44.750982Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss_sac(states, actor, critic1, alpha=0.4):\n",
    "    '''\n",
    "    Подсчет loss для актора.\n",
    "    \n",
    "    \n",
    "    Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        alpha: float\n",
    "        \n",
    "    Возвращает\n",
    "        -------\n",
    "        actor_loss: torch.tensor [batch_size]\n",
    "    '''\n",
    "\n",
    "    actions, log_prob = actor.apply(states)\n",
    "    qvalues = critic1.get_qvalues(states, actions) - alpha * log_prob\n",
    "    actor_loss = -qvalues.mean()\n",
    "    \n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пайплайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель &mdash; достичь в среднем хотя бы **300 награды**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:00:10.972995Z",
     "start_time": "2025-04-24T09:00:10.965573Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:45.939324Z",
     "iopub.status.busy": "2025-04-24T01:24:45.938599Z",
     "iopub.status.idle": "2025-04-24T01:24:45.966170Z",
     "shell.execute_reply": "2025-04-24T01:24:45.964739Z",
     "shell.execute_reply.started": "2025-04-24T01:24:45.939283Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42 # Иногда может сильно не повести\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:45:08.879108Z",
     "start_time": "2025-04-24T10:45:05.325186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 522663), started 0:00:01 ago. (Use '!kill 522663' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3bd52c776188572a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3bd52c776188572a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SAC(env, exp_replay, actor, critic1, target_critic1, critic2, target_critic2,\n",
    "            max_grad_norm=10, n_iter_max=1000000, timesteps_per_epoch=1, start_timesteps = 5000,\n",
    "            batch_size=128, policy_update_freq=2, gamma=0.99, tau=0.005, alpha=0.1):\n",
    "    \n",
    "    interaction_state, _ = env.reset()\n",
    "    random_actor = RandomActor()\n",
    "    \n",
    "    for n_iterations in trange(0, n_iter_max, timesteps_per_epoch):\n",
    "        \n",
    "        if len(exp_replay) < start_timesteps:\n",
    "            _, interaction_state = play_and_record(interaction_state, random_actor, env,\n",
    "                                                   exp_replay, timesteps_per_epoch)\n",
    "            continue\n",
    "\n",
    "        _, interaction_state = play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        states, actions, rewards, next_states, is_done = exp_replay.sample_random(batch_size)\n",
    "\n",
    "        states = torch.tensor(states, device=DEVICE, dtype=torch.float)\n",
    "        actions = torch.tensor(actions, device=DEVICE, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float)\n",
    "        next_states = torch.tensor(next_states, device=DEVICE, dtype=torch.float)\n",
    "        is_done = torch.tensor(is_done.astype('float32'), device=DEVICE, dtype=torch.float)\n",
    "        \n",
    "        critic_target = compute_critic_target_sac(rewards, next_states, is_done, actor, target_critic1, target_critic2, gamma, alpha)\n",
    "        critic1_predict = critic1.get_qvalues(states, actions)\n",
    "        critic1_loss = torch.nn.functional.mse_loss(critic1_predict, critic_target)\n",
    "        optimize(\"critic1\", critic1, opt_critic1, critic1_loss, n_iterations, max_grad_norm)\n",
    "        \n",
    "        critic2_predict = critic2.get_qvalues(states, actions)\n",
    "        critic2_loss = torch.nn.functional.mse_loss(critic2_predict, critic_target) \n",
    "        optimize(\"critic2\", critic2, opt_critic2, critic2_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        if n_iterations % policy_update_freq == 0:\n",
    "            actor_loss = compute_actor_loss_sac(states, actor, critic1, alpha)\n",
    "            optimize(\"actor\", actor, opt_actor, actor_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "            update_target_networks(critic1, target_critic1, tau)\n",
    "            update_target_networks(critic2, target_critic2, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:44:15.948407Z",
     "start_time": "2025-04-24T10:28:38.990752Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:24:54.056805Z",
     "iopub.status.busy": "2025-04-24T01:24:54.055942Z",
     "iopub.status.idle": "2025-04-24T01:25:05.550764Z",
     "shell.execute_reply": "2025-04-24T01:25:05.548246Z",
     "shell.execute_reply.started": "2025-04-24T01:24:54.056761Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656941d17bc243a9b1ad42cc3bd79b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68400/2331244515.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states).to(self.device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[355], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_SAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_replay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_critic1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_critic2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[354], line 32\u001b[0m, in \u001b[0;36mrun_SAC\u001b[0;34m(env, exp_replay, actor, critic1, target_critic1, critic2, target_critic2, max_grad_norm, n_iter_max, timesteps_per_epoch, start_timesteps, batch_size, policy_update_freq, gamma, tau, alpha)\u001b[0m\n\u001b[1;32m     30\u001b[0m critic2_predict \u001b[38;5;241m=\u001b[39m critic2\u001b[38;5;241m.\u001b[39mget_qvalues(states, actions)\n\u001b[1;32m     31\u001b[0m critic2_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(critic2_predict, critic_target) \n\u001b[0;32m---> 32\u001b[0m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcritic2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_critic2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic2_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iterations \u001b[38;5;241m%\u001b[39m policy_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m     actor_loss \u001b[38;5;241m=\u001b[39m compute_actor_loss_sac(states, actor, critic1, alpha)\n",
      "Cell \u001b[0;32mIn[184], line 9\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(name, model, optimizer, loss, n_iterations, max_grad_norm)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m----> 9\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m env\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(name, loss\u001b[38;5;241m.\u001b[39mitem(), n_iterations)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:54\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     52\u001b[0m norms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((device, _), ([grads], _)) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m foreach) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43m_has_foreach_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     55\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39m_foreach_norm(grads, norm_type))\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_foreach_utils.py:43\u001b[0m, in \u001b[0;36m_has_foreach_support\u001b[0;34m(tensors, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_has_foreach_support\u001b[39m(tensors: List[Tensor], device: torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[43m_get_foreach_kernels_supported_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_foreach_utils.py:8\u001b[0m, in \u001b[0;36m_get_foreach_kernels_supported_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrad_mode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m no_grad\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeAlias\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_foreach_kernels_supported_devices\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the device type list that supports foreach kernels.\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_SAC(env, exp_replay, actor, critic1, target_critic1, critic2, target_critic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), \"actor_sac.pth\")\n",
    "torch.save(critic1.state_dict(), \"critic1_sac.pth\")\n",
    "torch.save(critic2.state_dict(), \"critic2_sac.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = SAC_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "actor.load_state_dict(torch.load(\"actor_sac.pth\", weights_only=True))\n",
    "critic1.load_state_dict(torch.load(\"critic1_sac.pth\", weights_only=True))\n",
    "critic2.load_state_dict(torch.load(\"critic2_sac.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68400/3480693833.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/semyon/ML/RL/ContinuousControl/videos_SAC/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/semyon/ML/RL/ContinuousControl/videos_SAC/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/semyon/ML/RL/ContinuousControl/videos_SAC/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_SAC\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_SAC/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_SAC').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перебирал все гиперпараметры, обучал более 1кк шагов. Не может перейти порог в -80. \n",
    "\n",
    "Как будто просто нет exploration, но подкручивание гиперпараметров не помогло.\n",
    "\n",
    "Возможно я где-то ошибка в жизни("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.147830Z",
     "start_time": "2025-04-24T01:58:06.147825Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(1.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:46:58.036798Z",
     "start_time": "2025-04-24T10:46:52.730916Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:25:07.733343Z",
     "iopub.status.busy": "2025-04-24T01:25:07.731821Z",
     "iopub.status.idle": "2025-04-24T01:25:14.077824Z",
     "shell.execute_reply": "2025-04-24T01:25:14.075960Z",
     "shell.execute_reply.started": "2025-04-24T01:25:07.733282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68400/3480693833.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша нагрда: -72.04442342902415\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Нужно больше учить?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[359], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m score \u001b[38;5;241m=\u001b[39m sessions\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mВаша нагрда: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mНужно больше учить?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mОтлично!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Нужно больше учить?"
     ]
    }
   ],
   "source": [
    "sessions = evaluate(env, actor, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Ваша нагрда: {score}\")\n",
    "\n",
    "assert score >= 300, \"Нужно больше учить?\"\n",
    "print(\"Отлично!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:27:45.332491Z",
     "start_time": "2025-04-24T10:27:43.771771Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:25:16.541154Z",
     "iopub.status.busy": "2025-04-24T01:25:16.540399Z",
     "iopub.status.idle": "2025-04-24T01:25:17.605304Z",
     "shell.execute_reply": "2025-04-24T01:25:17.603707Z",
     "shell.execute_reply.started": "2025-04-24T01:25:16.541099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68400/3480693833.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/semyon/ML/RL/ContinuousControl/videos_SAC/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/semyon/ML/RL/ContinuousControl/videos_SAC/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/semyon/ML/RL/ContinuousControl/videos_SAC/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_SAC\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:27:45.336278Z",
     "start_time": "2025-04-24T10:27:45.333699Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-24T01:25:19.788101Z",
     "iopub.status.busy": "2025-04-24T01:25:19.787356Z",
     "iopub.status.idle": "2025-04-24T01:25:19.818028Z",
     "shell.execute_reply": "2025-04-24T01:25:19.816422Z",
     "shell.execute_reply.started": "2025-04-24T01:25:19.788058Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_SAC/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_SAC').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бонусное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(5 баллов)__ </span> Выбить хотя бы **300 награды** в режиме хардкора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red\"> __(Дообучал чекпоинт TD3 с первой части заданий)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"Hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_buffer_size = 10**5\n",
    "exp_replay = ReplayBuffer(max_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.load_state_dict(torch.load(\"actor_td3.pth\", weights_only=True))\n",
    "critic1.load_state_dict(torch.load(\"critic1_td3.pth\", weights_only=True))\n",
    "critic2.load_state_dict(torch.load(\"critic2_td3.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "target_critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "target_critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfac4db5f1394ceeb551f9f46ffae401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), \"actor_td3_hard_ft_500k.pth\")\n",
    "torch.save(critic1.state_dict(), \"critic1_td3_hard_ft_500k.pth\")\n",
    "torch.save(critic2.state_dict(), \"critic2_td3_hard_ft_500k.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=1e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=1e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e33b2ac93144e3f9f1e42c78116023f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), \"actor_td3_hard_ft_1m.pth\")\n",
    "torch.save(critic1.state_dict(), \"critic1_td3_hard_ft_1m.pth\")\n",
    "torch.save(critic2.state_dict(), \"critic2_td3_hard_ft_1m.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"Hard\")\n",
    "\n",
    "max_buffer_size = 10**5\n",
    "exp_replay = ReplayBuffer(max_buffer_size)\n",
    "\n",
    "actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "actor.load_state_dict(torch.load(\"actor_td3_hard_ft_1m.pth\", weights_only=True))\n",
    "critic1.load_state_dict(torch.load(\"critic1_td3_hard_ft_1m.pth\", weights_only=True))\n",
    "critic2.load_state_dict(torch.load(\"critic2_td3_hard_ft_1m.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша нагрда: 250.96098360544823\n"
     ]
    }
   ],
   "source": [
    "sessions = evaluate(env, actor, n_games=1)\n",
    "score = sessions.mean()\n",
    "print(f\"Ваша нагрда: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semyon/.local/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/semyon/ML/RL/ContinuousControl/videos_hard folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/semyon/ML/RL/ContinuousControl/videos_hard/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/semyon/ML/RL/ContinuousControl/videos_hard/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/semyon/ML/RL/ContinuousControl/videos_hard/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_hard\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_hard/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_hard').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
